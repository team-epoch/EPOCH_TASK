# Week3_Task_ì¬í¬

---

<aside>

<aside>

## **ğŸ“˜Â Title**

> â„¹ï¸Â *APA. ì¸ìš© ë°©ì‹ ê¶Œê³ *
> 
</aside>

- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020. https://arxiv.org/abs/2103.00020
</aside>

---

<aside>

<aside>

## **ğŸ“–Â Abstract**

> â„¹ï¸Â *ë³¸ì¸ì˜ ë°©ì‹ìœ¼ë¡œ ì¬í•´ì„ í•´ì£¼ì„¸ìš”. ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ ê¸ˆí•©ë‹ˆë‹¤.*
> 
</aside>

- auto-regressiveí•˜ê²Œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ í† í°ì„ í•˜ë‚˜ì˜ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë°›ì•„ë“¤ì—¬ transformerë¥¼ í•™ìŠµí•˜ëŠ” text-to-image ë°©ì‹ì„ ì œì•ˆ
- ì¶©ë¶„íˆ ë§ì€ ë°ì´í„°ì™€ í° ìŠ¤ì¼€ì¼ì„ ì‚¬ìš©í•  ë•Œ zero-shot ë°©ì‹ì˜ í‰ê°€ì— ìˆì–´ì„œ ì´ì „ì˜ ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ê³¼ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì„
</aside>

---

<aside>

<aside>

## **ğŸ’¡Â Introduction**

> **ğŸ“Â ë…¼ë¬¸ ì„ íƒì˜ ë™ê¸°**
> 
> 
> **âœ…Â ë…¼ë¬¸ì—ì„œ ë‹¤ë£¨ê³  ìˆëŠ” ì£¼ìš” ì—°êµ¬ ë¬¸ì œë‚˜ ì§ˆë¬¸**
> 
</aside>

**ğŸ“Â ë…¼ë¬¸ ì„ íƒì˜ ë™ê¸°**

- ì´ì „ì— ë¦¬ë·°í–ˆë˜ CLIP ëª¨ë¸ì´ í™œìš©ëœ ë…¼ë¬¸ì´ë¼ ê¸°ì¡´ì˜ ê°œë…ì´ ì–´ë–¤ ì‹ìœ¼ë¡œ í™œìš©ë˜ëŠ”ì§€ ê¶ê¸ˆ

**âœ…Â ë…¼ë¬¸ì—ì„œ ë‹¤ë£¨ê³  ìˆëŠ” ì£¼ìš” ì—°êµ¬ ë¬¸ì œë‚˜ ì§ˆë¬¸**

- auto-regressive transformerì„ ê¸°ë°˜ìœ¼ë¡œ í•œ text-to-image ì‘ì—…ì— ìˆì–´ ëª¨ë¸ ì‚¬ì´ì¦ˆì™€ ë°ì´í„°ë¥¼ ì˜ scale up í•˜ëŠ” ë°©ì‹ì´ ë§ì€ ì„±ê³¼ë¥¼ ê¸°ë¡. 
  -> ëª¨ë¸ê³¼ ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ì„ ê·¹í•œìœ¼ë¡œ í‚¤ì›Œ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼œë³´ì!
</aside>

---

<aside>

<aside>

## **ğŸ“šÂ Background**

> â„¹ï¸Â *ë…¼ë¬¸ì˜ ì£¼ì œì™€ ê´€ë ¨ëœ ê¸°ì¡´ ì—°êµ¬ë“¤ ë° ë°°ê²½ ì§€ì‹ì„ ì†Œê°œ*
> 
> 
> **ğŸ“Â Related Work 1**
> 
> **ğŸ“Â Related Work 2**
> 
</aside>

**ğŸ“Â *Autoencoder*

- ì£¼ì–´ì§„ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì ì¬ ê³µê°„ì— ì••ì¶•í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸ë¡œ ì••ì¶•í•˜ëŠ” encoder, ì••ì¶•ì„ í‘¸ëŠ” decoderë¡œ ë‚˜ë‰¨.
  
  ![image](https://github.com/user-attachments/assets/590d5fda-a767-4a56-a916-4a35c21e6fda)

    

**ğŸ“Â VAE(Variational Autoencoder)**

- í•™ìŠµ ì´ë¯¸ì§€ë¥¼ ë³€í™˜í•œ ì ì¬ ê³µê°„(z)ì„ ì˜ ì •ëœë˜ê²Œ ë°°ì¹˜í•œ Autoencoder
    
  ![image](https://github.com/user-attachments/assets/f38abbf2-290a-4ea6-a47d-80e8ec9a0e66)

    

**ğŸ“ dVAE(Discrete Variational Autoencoder) **
- ì´ë¯¸ì§€ì˜ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ì—°ì†ì ì¸ í”½ì…€ ê°’ ëŒ€ì‹  ì´ì‚°ì ì¸ ì½”ë“œ(visual tokens)ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸ë¡œ ë³€í™˜ ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ìŒ
  1) ì´ë¯¸ì§€ ë°ì´í„° encoderë¥¼ í†µí•´ ì ì¬ê³µê°„ìœ¼ë¡œ ë³€í™˜ 
  2)	ë¯¸ë¦¬ í•™ìŠµëœ visual codebookì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì½”ë“œ ì„ íƒí•´ ì´ì‚°ì ì¸ í† í°ìœ¼ë¡œ ë§¤í•‘ (256x256 í”½ì…€ ì´ë¯¸ì§€ -> 32x32 í† í° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜)


**ğŸ“ Visual codebook **
- ì›ë³¸ ì´ë¯¸ì§€ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•  ë•Œ ì°¸ì¡°ë˜ëŠ” ê²ƒìœ¼ë¡œ ì ì¬ ê³µê°„ ìƒì˜ ë²¡í„°ë“¤ì´ ê°ê° ì‹¤ì œ ì´ë¯¸ì§€ì˜ ì–´ë–¤ ë¶€ë¶„ê³¼ ë§¤ì¹­í•˜ëŠ”ì§€ ì •ë¦¬í•´ë†“ì€ ë¦¬ìŠ¤íŠ¸. 


**ğŸ“ ELB(Evidence Lower Bound, ì¦ê±° í•˜í•œ ê²½ê³„) **
- VAEì™€ ê°™ì€ ëª¨ë¸ì—ì„œ í•™ìŠµì„ ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•œ ì†ì‹¤í•¨ìˆ˜ë¡œ ëª¨ë¸ì´ ìµœì ì˜ í™•ë¥ ë¶„í¬ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì›€. 


**ğŸ“ BPE-encoded(Byte Pair Encoding) **
- ìì—°ì–´ ì²˜ë¦¬ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì••ì¶•í•˜ì—¬ í† í°í™”í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜. ì¼ë°˜ì ì¸ ë‹¨ì–´ ê¸°ë°˜ í† í°í™”ì™€ ë‹¬ë¦¬ ìì£¼ ë“±ì¥í•˜ëŠ” ë¬¸ì ìŒì„ ë³‘í•©í•´ ë” ì‘ì€ ë‹¨ìœ„ë¡œ subword í† í°ì„ ìƒì„± (Ex. lowest -> [â€œlowâ€, â€œestâ€])


**ğŸ“ BPE-encoded(Byte Pair Encoding) **
- ì‹ ê²½ë§ì—ì„œ ì—­ì „íŒŒ ê³¼ì • ì¤‘ ê¸°ìš¸ê¸°ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ëŠ” ê¸°ë²•

</aside>

---

<aside>

<aside>

## **ğŸ”Â Methods**

> **âœ…Â ì‚¬ìš©ëœ ì—°êµ¬ ë°©ë²•**
> 
> 
> **âœ…Â ì‹¤í—˜ ì„¤ê³„**
> 
> **ğŸ“Â ëª¨ë¸ ë¹„êµ** 
> 
</aside>

- ëª¨ë¸ íŒŒë¼ë¯¸í„° 120ì–µê°œ, í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìŒ ë°ì´í„° 2ì–µ 5ì²œë§Œ ì¥

- 256ê°œì˜ BPE-ì¸ì½”ë”© ëœ í…ìŠ¤íŠ¸ í† í°ê³¼ 32x32=1024 ì´ë¯¸ì§€ í† í°ì„ ê²°í•©í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ì— ì…ë ¥í•œ ë’¤ í™•ë¥ ë¶„í¬ë¥¼ í•™ìŠµ

- ì „ì²´ì ì¸ í•™ìŠµ ê³¼ì •ì€ ì´ë¯¸ì§€ x, ìº¡ì…˜ y, í† í° zì— ëŒ€í•œ ELB(evidence lower bound)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆìŒ.
  
  ![image](https://github.com/user-attachments/assets/956d74ba-d1c3-4aeb-8009-70fa38e72d1e)
  í™•ë¥ ë¶„í¬ ëª¨ë¸ë§
  
- lower bound
  
  ![image](https://github.com/user-attachments/assets/bd5bfe12-7798-4dea-abc6-185522bdc613)
  
  Phiì™€ thetaë¥¼ ê³ ì •í•œ ì±„ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ í† í°ì— ëŒ€í•œ prior distributionì„ í•™ìŠµ
  Phiì— ëŒ€í•œ ELBë¥¼ ê·¹ëŒ€í™”í•˜ë©° phiëŠ” 120ì–µ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ëŠ” sparse transformerë¥¼ ì‚¬ìš©
  
- Mixed-Precision Training
  GPU ë©”ëª¨ë¦¬ ì ˆì•½, throughput ì¦ê°€ë¥¼ ìœ„í•´ 16-bit precision parameter ì‚¬ìš©
  Underflowë¥¼ í”¼í•˜ë©° large scale ëª¨ë¸ì„ 16-bit precisionìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë„ë¡ í•˜ê¸° ì–´ë ¤ì›€ -> resblockì—ì„œ activationì˜ gradient normì´ ì¼ì •í•˜ê²Œ ë’¤ìª½ layerë¡œ ê°ˆìˆ˜ë¡ ê°ì†Œí•˜ë‹¤ê°€ 16-bitë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ ë¬¸ì œ ë°œìƒ
  solution -> resblockë§ˆë‹¤ gradient scale ì ìš©


</aside>

---

<aside>

<aside>

## **ğŸ”Â Experiments**

> **âœ…Â ë°ì´í„°ì…‹**
> 
> 
> **âœ…Â Models**
> 
> **âœ…Â Evaluation Metrics**
> 
> **âœ…Â Implementation Details**
> 
</aside>

- í‰ê°€ì§€í‘œ
  1) FID(Frechet Inception Distance): ìƒì„±ëœ ì´ë¯¸ì§€ì™€ ì‹¤ì œ ì´ë¯¸ì§€ì˜ ë¶„í¬ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ê±°ë¦¬
  2) IS(Inception Score): ìƒì„±ëœ ì´ë¯¸ì§€ì˜ ë‹¤ì–‘ì„±ê³¼ ì„ ëª…ë„ë¥¼ í‰ê°€í•˜ëŠ” ì§€í‘œ
  
  ![image](https://github.com/user-attachments/assets/d90f7c77-55b4-4217-8481-de4c4a5d9e39)


  (a) MS-COCO ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ ê¸°ì¡´ ëª¨ë¸ë“¤ë³´ë‹¤ FID, ISê°€ ë” ì¢‹ê²Œ ë‚˜ì˜´
  (b) CUB ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” DM-GAN, DF-GANë³´ë‹¤ FID ì„±ëŠ¥ì€ ë” ì•ˆì¢‹ê²Œ ISëŠ” ê°€ì¥ ì•ˆì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„
  (c) clipì—ê²Œ ë§ì€ candidatesë¥¼ ì£¼ê³  ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ë½‘ìœ¼ë©´ ì„±ëŠ¥ì´ í–¥ìƒ(32ê°œê¹Œì§€ FID, IS ìƒìŠ¹)

</aside>

---

<aside>

<aside>

## **ğŸ“–Â Conclusion**

> **âœ…Â Limitation**
> 
> 
> **âœ…Â Contribution**
> 
</aside>

- Autoregressive transformerì„ ê¸°ë°˜ìœ¼ë¡œ í•œ text-to-image generationì— ëŒ€í•œ ê°„ë‹¨í•œ ì ‘ê·¼ë²•ì„ ì—°êµ¬
- Model, data scaleì„ ë†’ì´ëŠ” ê²ƒì´ zero-shot, ë‹¨ì¼ ìƒì„±í˜• ëª¨ë¸ ê´€ì ì—ì„œ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒ

</aside>

---

<aside>

<aside>

## **ğŸ“šÂ í•˜í–¥ì‹ ì ‘ê·¼**

> â„¹ï¸Â *ê³µí†µ ë…¼ë¬¸ í•™ìŠµ ì¤‘ ëª°ëë˜ ê°œë…, ìš©ì–´ì— ëŒ€í•œ ë…¼ë¬¸ì„ ì¶”ê°€ë¡œ ì°¾ì•„ë´…ì‹œë‹¤!*
> 
> 
> â„¹ï¸Â *í•œ ë„ë©”ì¸ì˜ ëª¨ë¥´ëŠ” ë‚´ìš©ì— ëŒ€í•´ **í•˜í–¥ì‹**ìœ¼ë¡œ ì ‘ê·¼í•˜ë‹¤ë³´ë©´ ê´€ë ¨ ë…¼ë¬¸ë“¤ì´ ì ì°¨ ì‰½ê²Œ ì½íˆê²Œ ë©ë‹ˆë‹¤.* 
> 
> **ğŸ“Â e.g.** `transformer`ë…¼ë¬¸ì„ ë³´ë‹¤ê°€ `sequence to sequence`ê°€ ë­”ì§€ ëª°ë¼ ì°¾ì•„ì„œ ê³µë¶€
> 
> **ğŸ“Â e.g.**`sequence to sequence` ë…¼ë¬¸ì„ ë³´ë‹¤ê°€ `LSTM`ì´ ë­”ì§€ ëª°ë¼ ì°¾ì•„ì„œ ê³µë¶€
> 
</aside>

- background ë¶€ë¶„ ì°¸ê³ 

</aside>

---

<aside>

<aside>

## **ğŸ¤”Â Question**

> â„¹ï¸Â *ë³¸ì¸ì´ ìˆ˜í–‰í•œ í•™ìŠµì— ëŒ€í•´ ìŠ¤ìŠ¤ë¡œ ì§ˆë¬¸í•˜ê³  ë‹µí•´ë³´ì„¸ìš”.*
> 
> 
> **ğŸ“Â ì´ ë…¼ë¬¸ì´ ë“±ì¥í•˜ê²Œ ëœ ì´ìœ  + ì´ ë…¼ë¬¸ì´ ê´€ë ¨ Taskì— ê¸°ì—¬í•œ ë‚´ìš©**
> 
> **ğŸ“Â ë°°ìš¸ ìˆ˜ ìˆì—ˆë˜ ë‚´ìš©ê³¼ ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì **
> 
> **ğŸ“Â Git-Hubì— ê³µê°œëœ ì½”ë“œë¥¼ ë³´ê³  ì´í•´í•œ ë°” `(Opt.)`**
> 
</aside>

- ëŒ€ê·œëª¨ ëª¨ë¸, ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµì‹œí‚¬ ë•Œ ì–´ë–¤ ë¶€ë¶„ì—ì„œ ì–´ë ¤ì›€ì´ ìˆê³  ì´ë¥¼ í•´ê²·í•˜ëŠ” ê¸°ë²•ë“¤ì— ëŒ€í•´ ì•Œê²Œ ë˜ì—ˆìŒ. 
- êµ¬ì²´ì ì¸ ëª¨ë¸ ì•„í‚¤í…ì³ê°€ ë“œëŸ¬ë‚˜ì§€ ì•Šì•„ ì•„ì‰¬ìš´ ë¶€ë¶„ì´ ìˆìŒ

</aside>

---

<aside>

<aside>

## **ğŸ¤”Â Next Task**

> â„¹ï¸Â *ì°¨ì£¼ì— ë¦¬ë·° ì˜ˆì •ì¸ ë…¼ë¬¸ ê¸°ì¬(APA. ì¸ìš© ë°©ì‹ ê¶Œê³ )*
> 
</aside>

- Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR) (pp. 10684-10695)

</aside>

---
