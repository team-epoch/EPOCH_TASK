# Week2_Task_ìš°ë¦¼

---

<aside>

<aside>

## **ğŸ“˜Â Title**

> â„¹ï¸Â *APA. ì¸ìš© ë°©ì‹ ê¶Œê³ *
> 
</aside>

- Hasselt, H. V., Guez, A., & Silver, D. (2015). Deep reinforcement learning with double Q-learning.Â *Proceedings of the 2015 AAAI Conference on Artificial Intelligence*, 2094-2100.Â [https://arxiv.org/abs/1509.06461](https://arxiv.org/abs/1509.06461)
</aside>

---

<aside>

<aside>

## **ğŸ“–Â Abstract**

> â„¹ï¸Â *ë³¸ì¸ì˜ ë°©ì‹ìœ¼ë¡œ ì¬í•´ì„ í•´ì£¼ì„¸ìš”. ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ ê¸ˆí•©ë‹ˆë‹¤.*
> 
</aside>

- DDQN(Deep Double Q-learning): DQN(Double Q-learning) ì•Œê³ ë¦¬ì¦˜ + ì‹¬ì¸µ ê°•í™”í•™ìŠµ ê²°í•©í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆ
    - Q-learning: ì¸ê¸° ìˆëŠ” ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì´ì§€ë§Œ action valuesë¥¼ overestimate í•˜ëŠ” ê²½í–¥ ì¡´ì¬
    - DQN(Double Q-learning): ë§ˆì°¬ê°€ì§€ë¡œ í•™ìŠµ ê³¼ì •ì—ì„œ overestimate í•  ê²½í–¥ì´ í¬ê³  ì œí•œëœ í™˜ê²½ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥
- â“Â Overestimation ë°œìƒ ì´ìœ : target Q-value ê³„ì‚°ì‹œ max ê°’ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸
- ì´ì „ ì—°êµ¬ì—ì„œì˜ ë¬¸ì œì (overestimation; ê³¼ëŒ€í‰ê°€)ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë“±ì¥í•œ ì•Œê³ ë¦¬ì¦˜. ì™„ì „íˆ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì´ë¼ê¸°ë³´ë‹¤ëŠ” Q-learning, DQNì„ ì¡°ê¸ˆì”© ë³€í˜•í•˜ì—¬ ê³¼ëŒ€í‰ê°€ ë¬¸ì œë¥¼ ì¤„ì´ê³ , Atari 2600 ê²Œì„ í™˜ê²½ì—ì„œì˜ ì‹¤í—˜ì„ í†µí•´ DQNë³´ë‹¤ ì¢‹ì•„ì§„ ì„±ëŠ¥ì„ ë³´ì„.
</aside>

---

<aside>

<aside>

## **ğŸ’¡Â Introduction**

> **ğŸ“Â ë…¼ë¬¸ ì„ íƒì˜ ë™ê¸°**
> 
> 
> **âœ…Â ë…¼ë¬¸ì—ì„œ ë‹¤ë£¨ê³  ìˆëŠ” ì£¼ìš” ì—°êµ¬ ë¬¸ì œë‚˜ ì§ˆë¬¸**
> 
</aside>

**ğŸ“Â ë…¼ë¬¸ ì„ íƒì˜ ë™ê¸°**

- ê°•í™”í•™ìŠµ ë°œì „ ê³¼ì •ì— ë”°ë¼ ë…¼ë¬¸ì„ ë¦¬ë·°í•˜ê³  ì‹¶ì—ˆìœ¼ë‚˜â€¦ DDQN ì „ì— DQNì´ ìˆì—ˆë‹¤ (week3 ë¦¬ë·° ì˜ˆì •)
- ì¸ê¸° ìˆëŠ” Q-learning ì•Œê³ ë¦¬ì¦˜ ì´í›„ ë°œì „, ë³€í˜•ëœ ì•Œê³ ë¦¬ì¦˜ì„ ê³µë¶€í•˜ê³ ì ì„ íƒ

**âœ…Â ë…¼ë¬¸ì—ì„œ ë‹¤ë£¨ê³  ìˆëŠ” ì£¼ìš” ì—°êµ¬ ë¬¸ì œë‚˜ ì§ˆë¬¸**

- Q-learningì˜ overestimation(ê³¼ëŒ€í‰ê°€) ì´ìŠˆë¥¼ ì–´ë–»ê²Œ í•´ê²°?
- í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” DDQNì€ Atari 2600ì— ëŒ€í•´ ê¸°ì¡´ DQN ëŒ€ë¹„ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ”ì§€ ë³´ì´ê³ ì í•¨
</aside>

---

<aside>

<aside>

## **ğŸ“šÂ Background**

> â„¹ï¸Â *ë…¼ë¬¸ì˜ ì£¼ì œì™€ ê´€ë ¨ëœ ê¸°ì¡´ ì—°êµ¬ë“¤ ë° ë°°ê²½ ì§€ì‹ì„ ì†Œê°œ*
> 
> 
> **ğŸ“Â Related Work 1**
> 
> **ğŸ“Â Related Work 2**
> 
</aside>

**ğŸ“Â Q-learning**

- ì£¼ì–´ì§„ policyÂ $\pi$ì— ëŒ€í•´ state sì—ì„œ action aì˜ true value
- Optimal policy: ê° stateì—ì„œ ê°€ì¥ ë†’ì€ Q-value ì„ íƒ
    
    ![image.png](Week2_Task_%E1%84%8B%E1%85%AE%E1%84%85%E1%85%B5%E1%86%B7%201b204fc3e65180e59512fe7ef28fc26a/image.png)
    
- ë‹¨ì : ëª¨ë“  statesì—ì„œ ëª¨ë“  action valuesë¥¼ í•™ìŠµí•˜ê¸°ì—ëŠ” ì—°ì‚°ëŸ‰ì´ ë„ˆë¬´ ë§ìŒ
    
    â†’ ëŒ€ì‹  Q-valueë¥¼ $\theta$ë¡œ íŒŒë¼ë¯¸í„°í™”í•˜ì—¬ í•™ìŠµ!
    
    ![image.png](Week2_Task_%E1%84%8B%E1%85%AE%E1%84%85%E1%85%B5%E1%86%B7%201b204fc3e65180e59512fe7ef28fc26a/image%201.png)
    
- targetì„ ë‹¤ìŒê³¼ ê°™ì˜ ì •ì˜:
    
    ![image.png](Week2_Task_%E1%84%8B%E1%85%AE%E1%84%85%E1%85%B5%E1%86%B7%201b204fc3e65180e59512fe7ef28fc26a/2fe0e357-396f-43d9-a466-8e123db46462.png)
    

**ğŸ“Â Deep Q Network**

- Q-learning + multi-layered neural network
- target networkë¥¼ ì‚¬ìš©í•˜ì—¬ $\theta$ ëŒ€ì‹  $\theta^-$ ì‚¬ìš©
    
    ![image.png](Week2_Task_%E1%84%8B%E1%85%AE%E1%84%85%E1%85%B5%E1%86%B7%201b204fc3e65180e59512fe7ef28fc26a/image%202.png)
    

**ğŸ“Â Double Q-learning**

- Q-learning, DQNì—ì„œì˜ max operatorëŠ” actionì„ ì„ íƒ, í‰ê°€í•˜ëŠ” ë° ê°™ì€ ê°’ì„ ì‚¬ìš©í•˜ë©° ì´ëŠ” overestimate ì•¼ê¸° ê°€ëŠ¥ â†’ ë‘ ê°œì˜ Q-í•¨ìˆ˜ ì‚¬ìš©
- $\theta$, $\theta'$ ë‘˜ ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤í•˜ê²Œ ì„ íƒí•˜ì—¬ ì—…ë°ì´íŠ¸
    
    ![image.png](Week2_Task_%E1%84%8B%E1%85%AE%E1%84%85%E1%85%B5%E1%86%B7%201b204fc3e65180e59512fe7ef28fc26a/image%203.png)
    
- í•˜ë‚˜ì˜ Q-í•¨ìˆ˜ ì‚¬ìš©: action ì„ íƒ
- ë‹¤ë¥¸ Q-í•¨ìˆ˜ ì‚¬ìš©: í•´ë‹¹ action valueë¥¼ í‰ê°€
</aside>

---

<aside>

<aside>

## **ğŸ”Â Methods**

> **âœ…Â ì‚¬ìš©ëœ ì—°êµ¬ ë°©ë²•**
> 
> 
> **âœ…Â ì‹¤í—˜ ì„¤ê³„**
> 
> **ğŸ“Â ëª¨ë¸ ë¹„êµ** 
> 
</aside>

- Double DQN: Double Q-learning + DQN ì´ë¼ê³  ë³´ë©´ ë  ê²ƒ ê°™ìŒ
    - ì—…ë°ì´íŠ¸ëŠ” DQNê³¼ ë™ì¼í•œ ë°©ì‹ì´ë‚˜ targetì„ ë‹¤ìŒìœ¼ë¡œ ëŒ€ì²´:
        
        ![image.png](Week2_Task_%E1%84%8B%E1%85%AE%E1%84%85%E1%85%B5%E1%86%B7%201b204fc3e65180e59512fe7ef28fc26a/image%204.png)
        
    - ë‘ ë²ˆì§¸ networkë¡œ target networkë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ ì‚¬ìš©
    - DQN í˜•íƒœë¥¼ ìœ ì§€í•˜ë©´ì„œ Double Q-learningì˜ ì¥ì ì„ ìµœëŒ€í•œ í™œìš©
</aside>

---

<aside>

<aside>

## **ğŸ”Â Experiments**

> **âœ…Â ë°ì´í„°ì…‹**
> 
> 
> **âœ…Â Models**
> 
> **âœ…Â Evaluation Metrics**
> 
> **âœ…Â Implementation Details**
> 
</aside>

- DQNì˜ overestimation ë¶„ì„ + Double DQNì´ value accuracyì™€ policy quality ì¸¡ë©´ì—ì„œ DQNë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ ì¤Œ
- Datasets: Atari 2600 games
- Experimental setting, network architecture: DQN paper*ì—ì„œì˜ ì•„ì›ƒë¼ì¸ì„ ë”°ë¦„
- ë„¤íŠ¸ì›Œí¬ëŠ” ë§ˆì§€ë§‰ ë„¤ í”„ë ˆì„ì„ ì¸í’‹ìœ¼ë¡œ ë°›ì•„ì„œ ê° ì•¡ì…˜ì˜ ì•¡ì…˜ ê°’ì„ ì¶œë ¥
- ê° ê²Œì„ì—ì„œ 200ë§Œ í”„ë ˆì„(ì•½ 1ì£¼ì¼ ë™ì•ˆ) ë‹¨ì¼ GPUë¡œ ë„¤íŠ¸ì›Œí¬ í›ˆë ¨
- í‰ê°€ ì§€í‘œ: value estimate
- Results on overoptimism
    - 6ê°œì˜ Atari ê²Œì„ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼
    - Learning curve of DQN: true valueë³´ë‹¤ ë†’ì€ ìœ„ì¹˜
    - Learning curve of DDQN: true valueì™€ í›¨ì”¬ ê°€ê¹Œì›€
        - DDQNì´ ë” ì •í™•í•œ value estimate, ë” ë‚˜ì€ policyë¥¼ ë§Œë“¤ì–´ëƒ„

* Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level control through deep reinforcement learning. *Nature, 518*(7540), 529-533. https://doi.org/10.1038/nature14236

</aside>

---

<aside>

<aside>

## **ğŸ“–Â Conclusion**

> **âœ…Â Limitation**
> 
> 
> **âœ…Â Contribution**
> 
</aside>

- Q-learningì´ large-scale ë¬¸ì œì—ì„œë„ ì§€ë‚˜ì¹˜ê²Œ ë‚™ê´€ì ì¼ ìˆ˜ ìˆëŠ” ì´ìœ ë¥¼ ë³´ì—¬ ì¤Œ
- Atari ê²Œì„ì—ì„œì˜ value ê³¼ëŒ€í‰ê°€ ë¬¸ì œê°€ ì´ì „ì— ì¸ì •ëœ ê²ƒë³´ë‹¤ ì‹¤ì œë¡œ ë” í”í•˜ê³  ì‹¬ê°í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ ì¤Œ
- DDQNì´ ê³¼ëŒ€í‰ê°€ ë¬¸ì œë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¤„ì´ê³  ì•ˆì •ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ ì¤Œ
- ì¶”ê°€ì ì¸ ë„¤íŠ¸ì›Œí¬ë‚˜ ëª¨ìˆ˜ë¥¼ í•„ìš”ë¡œ í•˜ì§€ ì•Šê³  ê¸°ì¡´ ì•„í‚¤í…ì²˜+ì‹¬ì¸µì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ DDQNì´ë¼ëŠ” íŠ¹ì • ë°©ë²•ë¡  ì œì•ˆ
- DDQNì´ ë” ë‚˜ì€ policyë¥¼ ì°¾ëŠ”ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ ì¤Œ
</aside>

---

<aside>

<aside>

## **ğŸ“šÂ í•˜í–¥ì‹ ì ‘ê·¼**

> â„¹ï¸Â *ê³µí†µ ë…¼ë¬¸ í•™ìŠµ ì¤‘ ëª°ëë˜ ê°œë…, ìš©ì–´ì— ëŒ€í•œ ë…¼ë¬¸ì„ ì¶”ê°€ë¡œ ì°¾ì•„ë´…ì‹œë‹¤!*
> 
> 
> â„¹ï¸Â *í•œ ë„ë©”ì¸ì˜ ëª¨ë¥´ëŠ” ë‚´ìš©ì— ëŒ€í•´ **í•˜í–¥ì‹**ìœ¼ë¡œ ì ‘ê·¼í•˜ë‹¤ë³´ë©´ ê´€ë ¨ ë…¼ë¬¸ë“¤ì´ ì ì°¨ ì‰½ê²Œ ì½íˆê²Œ ë©ë‹ˆë‹¤.* 
> 
> **ğŸ“Â e.g.** `transformer`ë…¼ë¬¸ì„ ë³´ë‹¤ê°€ `sequence to sequence`ê°€ ë­”ì§€ ëª°ë¼ ì°¾ì•„ì„œ ê³µë¶€
> 
> **ğŸ“Â e.g.**`sequence to sequence` ë…¼ë¬¸ì„ ë³´ë‹¤ê°€ `LSTM`ì´ ë­”ì§€ ëª°ë¼ ì°¾ì•„ì„œ ê³µë¶€
> 
</aside>

- ê°•í™”í•™ìŠµì—ì„œ ì‚¬ìš©ë˜ëŠ” ìš©ì–´ ë° ì „ë°˜ì ì¸ ê°œë… ìµí˜
</aside>

---

<aside>

<aside>

## **ğŸ¤”Â Question**

> â„¹ï¸Â *ë³¸ì¸ì´ ìˆ˜í–‰í•œ í•™ìŠµì— ëŒ€í•´ ìŠ¤ìŠ¤ë¡œ ì§ˆë¬¸í•˜ê³  ë‹µí•´ë³´ì„¸ìš”.*
> 
> 
> **ğŸ“Â ì´ ë…¼ë¬¸ì´ ë“±ì¥í•˜ê²Œ ëœ ì´ìœ  + ì´ ë…¼ë¬¸ì´ ê´€ë ¨ Taskì— ê¸°ì—¬í•œ ë‚´ìš©**
> 
> **ğŸ“Â ë°°ìš¸ ìˆ˜ ìˆì—ˆë˜ ë‚´ìš©ê³¼ ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì **
> 
> **ğŸ“Â Git-Hubì— ê³µê°œëœ ì½”ë“œë¥¼ ë³´ê³  ì´í•´í•œ ë°” `(Opt.)`**
> 
</aside>

- ë³¸ ì—°êµ¬ëŠ” ì´ì „ ì—°êµ¬ì˜ í•œê³„ì ì„ ë³´ì™„í•˜ê³ ì ë“±ì¥í•˜ì˜€ìœ¼ë©° ì´ë¯¸ ê°œë°œëœ ë°©ë²•ë¡ (Q-learning ë° DQN)ì„ í™œìš©/ë³€í˜•í•˜ì—¬ ì¶”ê°€ì ì¸ ë„¤íŠ¸ì›Œí¬ë‚˜ ëª¨ìˆ˜ë¥¼ ìš”êµ¬í•˜ì§€ ì•Šìœ¼ë©´ì„œë„ ê³¼ëŒ€í‰ê°€ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì„ ì œì‹œí•˜ì˜€ë‹¤.
- ê°•í™”í•™ìŠµì˜ ì´ˆê¸° ë°œì „ ê³¼ì •ì— ë”°ë¥¸ ë°©ë²•ë¡ ì„ ê°„ë‹¨í•˜ê²Œë‚˜ë§ˆ ë°°ìš¸ ìˆ˜ ìˆì—ˆìŒ.
- ì‚¬ì‹¤ ê°•í™”í•™ìŠµì— ëŒ€í•´ ê¹Šì´ ê³µë¶€í•´ ë³¸ ì ì´ ì—†ê³  ë°±ê·¸ë¼ìš´ë“œ ì§€ì‹ì´ ì—†ì–´ì„œ ì´ë²ˆ ë…¼ë¬¸ì„ ì½ëŠ” ë° ë§ì´ ì• ë¨¹ì—ˆìŒâ€¦â€¦ *~~(backgroundì—ì„œ ì–¸ê¸‰ëœ Q-learning, DQN ëª¨ë‘ ì²¨ ë´¤ì–´ìš” ã…ã…â€¦)~~* ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì§€ë„í•™ìŠµê³¼ëŠ” ë‹¬ë¦¬ ê°•í™”í•™ìŠµì€ **ì •ë‹µì´ ì—†ê³ **, ë³´ìƒì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ í–‰ë™ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ë¼ëŠ” ì ì—ì„œ ê°™ì€ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì•¼ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  ê²°ì´ ë§ì´ ë‹¤ë¥´ë‹¤ê³  ëŠê¼ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì•ì„œ ì–¸ê¸‰í•œ ë…¼ë¬¸ë“¤ì„ ë¨¼ì € ì½ì–´ ë³´ê³  ë³¸ ë…¼ë¬¸ì„ ë‹¤ì‹œ ë¦¬ë”©í•œë‹¤ë©´ ì²˜ìŒë³´ë‹¤ ì´í•´í•˜ê¸° ìˆ˜ì›”í•  ê²ƒ ê°™ë‹¤. ë¹ ë¥¸ ì‹œì¼ ë‚´ì— ëª¨ë“  ê³µí†µ ë…¼ë¬¸ì„ ì½ì–´ì•¼ê² ë‹¤ê³  ë‹¤ì§í•˜ëŠ” ê³„ê¸°ê°€ ë˜ê¸°ë„â€¦. ê·¸ë˜ë„ ë‚´ê°€ ì˜ ì•Œì§€ ëª»í–ˆë˜ ìƒˆë¡œìš´ ë¶„ì•¼ë¥¼ ê³µë¶€í•˜ê³  ì•Œì•„ê°„ë‹¤ëŠ” ê±´ ì¦ê²ë‹¤. â¤ï¸
</aside>

---

<aside>

<aside>

## **ğŸ¤”Â Next Task**

> â„¹ï¸Â *ì°¨ì£¼ì— ë¦¬ë·° ì˜ˆì •ì¸ ë…¼ë¬¸ ê¸°ì¬(APA. ì¸ìš© ë°©ì‹ ê¶Œê³ )*
> 
</aside>

- Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level control through deep reinforcement learning. *Nature, 518*(7540), 529-533. https://doi.org/10.1038/nature14236

</aside>

---
