<aside>

<aside>

## **ğŸ“˜Â Title**

> â„¹ï¸Â *APA. ì¸ìš© ë°©ì‹ ê¶Œê³ *
> 
</aside>

- Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3431-3440. https://doi.org/10.1109/CVPR.2015.7298965
</aside>

---

<aside>

<aside>

## **ğŸ“–Â Abstract**

> â„¹ï¸Â *ë³¸ì¸ì˜ ë°©ì‹ìœ¼ë¡œ ì¬í•´ì„ í•´ì£¼ì„¸ìš”. ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ ê¸ˆí•©ë‹ˆë‹¤.*
> 
</aside>

- FCNì€ End-to-End, Pixel-to-Pixel í•™ìŠµì´ ê°€ëŠ¥í•˜ë„ë¡ í•œ sematic segmentation ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë„ì…ëœ ëª¨ë¸ì´ë‹¤. 
- ê¸°ì¡´ CNN ëª¨ë¸ì„ Fully Convolutional í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ End-to-End, Pixel-to-Pixel í•™ìŠµì´ ê°€ëŠ¥í•˜ë„ë¡ í–ˆë‹¤.
- ìƒˆë¡œìš´ skip architectureì„ ì‚¬ìš©í•˜ì—¬ ê¹Šì€ ì¸µê³¼ ì–•ì€ ì¸µì„ ê²°í•©í•˜ì—¬ "ì •í™•í•œ segmenatationì„ ê°€ëŠ¥"í•˜ë„ë¡ í–ˆë‹¤ëŠ” ê²ƒì´ ì£¼ìš” ë‚´ìš©ì´ë‹¤.

</aside>

---

<aside>

<aside>

## **ğŸ’¡Â Introduction**

</aside>

**ğŸ“Â ë…¼ë¬¸ ì„ íƒì˜ ë™ê¸°**

- ì§€ë‚œ ì£¼ì— ë¦¬ë·°í•œ U-Netì´ FCN ëª¨ë¸ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ê²ƒì´ë¼ê³  í•´ì„œ, ê¸°ì¡´ ëª¨ë¸ì¸ FCNì— ëŒ€í•´ ì¢€ ë” ê¹Šê²Œ ì•Œì•„ë³´ê³ ì ì„ íƒí–ˆë‹¤.

**âœ…Â ë…¼ë¬¸ì—ì„œ ë‹¤ë£¨ê³  ìˆëŠ” ì£¼ìš” ì—°êµ¬ ë¬¸ì œë‚˜ ì§ˆë¬¸**

- ê¸°ì¡´ CNN ê¸°ë°˜ classification ë„¤íŠ¸ì›Œí¬ë¥¼ ì–´ë–»ê²Œ ë³€í˜•í–ˆëŠ”ê°€?
- down-samplingìœ¼ë¡œ ì¸í•´ coarseí•œ feature mapì„ ì–´ë–»ê²Œ ë³µì›í•˜ì—¬ segmentation í•´ìƒë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆëŠ”ê°€?
- Deep feature hierarchyì—ì„œ skip architectureë¥¼ ì ìš©í•œë‹¤ë©´ segmentation ì„±ëŠ¥ì´ í–¥ìƒë ê¹Œ?

</aside>

---

<aside>

<aside>

## **ğŸ“šÂ Background**

> â„¹ï¸Â *ë…¼ë¬¸ì˜ ì£¼ì œì™€ ê´€ë ¨ëœ ê¸°ì¡´ ì—°êµ¬ë“¤ ë° ë°°ê²½ ì§€ì‹ì„ ì†Œê°œ*
> 
</aside>

1. ğŸ“Â Related Work : ê¸°ì¡´ì˜ classification ëª¨ë¸(AlexNet, VGGNet, GoogLeNet)ë“¤ì€ pixel ë‹¨ìœ„ì˜ ì˜ˆì¸¡(ì¦‰, segmentation)ì—ëŠ” ì ì ˆí•˜ì§€ ì•Šì•˜ë‹¤.
2. ğŸ“Â Related Work : sliding window detectionì„ í†µí•´ segmentationì„ ìˆ˜í–‰í–ˆì§€ë§Œ, fully convolutional ë°©ì‹ì€ ì•„ë‹ˆì—ˆìŒ(OverFeat (2014))
3. ğŸ“Â Related Work : Patchwise Training ë°©ë²•ì´ ë„ë¦¬ ì‚¬ìš©ë˜ì—ˆìœ¼ë‚˜, ì—°ì‚°ëŸ‰ì´ ë§ê³  ê³µê°„ì  ì§€ì†ì„±ì´ë¼ëŠ” í•œê³„ê°€ ìˆë‹¤. (Farabet et al. (2013), Pinheiro & Collobert (2014))
</aside>

---

<aside>

<aside>

## **ğŸ”Â Methods**

</aside>

**âœ…Â ì‚¬ìš©ëœ ì—°êµ¬ ë°©ë²•**

- ê¸°ì¡´ CNN ë¶„ë¥˜ ëª¨ë¸ì„ **fully convolutional**ë¡œ ë³€í˜• (Fully Connected Layer -> 1x1 Convolutional Layer)
- Feature Map í•´ìƒë„ ë³µì›ì„ ìœ„í•œ **Deconvolution Layer (Upsampling)** ì‚¬ìš©
- patchwise training ëŒ€ì‹  **whole image training** ì ìš©
- ë‹¤ì–‘í•œ feature layerë¥¼ ê²°í•©í•˜ëŠ” **Skip Architecture** ì„¤ê³„

**âœ…Â ì‹¤í—˜ ì„¤ê³„**

**ğŸ“Â ëª¨ë¸ ë¹„êµ**

1. **FCN vs. ê¸°ì¡´ CNN ëª¨ë¸(AlexNet, VGGNet, GoogLeNet)**
    
2. **Shift-and-Stichë°©ë²• vs. Deconvolutionë°©ë²•**
 
</aside>

---

<aside>

<aside>

## **ğŸ”Â Experiments**

</aside>

**âœ…Â ë°ì´í„°ì…‹**

- **PASCAL VOC 2011 & 2012**
- **NYUDv2 (RGB-D ë°ì´í„°ì…‹)**
- **SIFT Flow (Scene Parsing ë°ì´í„°ì…‹)**

**âœ…Â Models**

ê° layer
- **FCN-32s**
- **FCN-16s**
- **FCN-8s**

**âœ…Â Evaluation Metrics**

- **Pixel Accuracy**: ì „ì²´ í”½ì…€ ì¤‘ ì •í™•í•˜ê²Œ ë¶„ë¥˜ëœ ë¹„ìœ¨
- **Mean Accuracy**: í´ë˜ìŠ¤ë³„ ì •í™•ë„ì˜ í‰ê· 
- **Mean IoU**: IoUì˜ í´ë˜ìŠ¤ë³„ í‰ê· 
- **Frequency Weighted IoU**: í´ë˜ìŠ¤ ë¹ˆë„ì˜ IoU

**âœ…Â Implementation Details**

- **Loss Function**: Per-pixel Multinomial Logistic Loss (Softmax Loss) ì‚¬ìš©
- **Optimizer**: Stochastic Gradient Descent (SGD)
- **Fine-tuning**: ê¸°ì¡´ CNN ë¶„ë¥˜ ëª¨ë¸ì—ì„œ ì‚¬ì „ í•™ìŠµëœ weightë¥¼ ì´ˆê¸°í™”, ì¶”ê°€ segmentation layerì—ëŠ” ì´ˆê¸° weightë¥¼ 0ìœ¼ë¡œ ì„¤ì •
- **Patchwise Training vs. whole Image Training**
- **í•™ìŠµì‹œê°„**: FCN-32sëŠ” 3ì¼ í•™ìŠµ, FCN-16s & FCN-8sëŠ” 4ì¼ í•™ìŠµ

**âœ…Â ì‹¤í—˜ ê²°ê³¼**

- **PASCAL VOC 2012 ê²°ê³¼(SOTA ë¹„êµ)**
    - **R-CNN**: Mean IU: 47.9%, ì¶”ë¡  ì†ë„: ëŠë¦¼
    - **SDS (ê¸°ì¡´ SOTA)**: Mean IU: 52.6%, ì¶”ë¡  ì†ë„: 50ì´ˆ
    - **FCN-32s**: Mean IU: 59.4%, Pixel Accuracy: 89.1%, ì¶”ë¡  ì†ë„: 210ms
    - **FCN-16s**: Mean IU: 62.4%, Pixel Accuracy: 90%, ì¶”ë¡  ì†ë„: 210ms
    - **FCN-8s**: Mean IU: 62.7%, Pixel Accuracy: 90.3%, ì¶”ë¡  ì†ë„: 175ms
  â–¶ï¸ FCN-8sê°€ ê¸°ì¡´ SOTA ëŒ€ë¹„ ì•½ 20% í–¥ìƒëœ Mean IU ì„±ëŠ¥ì„ ë‹¬ì„±
  â–¶ï¸ FCNì€ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ì¶”ë¡  ì†ë„ê°€ í›¨ì”¬ ë¹ ë¦„

- **NYUDv2 ê²°ê³¼**
    - **FCN-32s (RGB)**: Mean IU: 29.2%, Mean Accuracy: 42.2%, Pixel Accuracy: 60% 
    - **FCN-16s (RGB + HHA)**: Mean IU: 34%, Mean Accuracy: 46.1%, Pixel Accuracy: 65.4%
  â–¶ï¸ RGB-D ë°ì´í„°ë¥¼ í™œìš©í•˜ë©´ segmentation ì„±ëŠ¥ì´ í–¥ìƒë¨
  â–¶ï¸ HHA Encodingì„ ì¶”ê°€í•œ ê²½ìš° ì„±ëŠ¥ì´ ê°€ì¥ ìš°ìˆ˜í•¨

</aside>

---

<aside>

<aside>

## **ğŸ“–Â Conclusion**

</aside>

**âœ…Â Limitation**

- **í•´ìƒë„ loss ë¬¸ì œ**: Poolingê³¼ Strideë¡œ ì¸í•´ fine detailì´ ì†ì‹¤ë  ìˆ˜ ìˆìŒ
- **global context ì •ë³´ ë¶€ì¡±**: Long-range dependencyë¥¼ ê³ ë ¤í•˜ê¸° ì–´ë ¤ì›€
- **ì—°ì‚°ëŸ‰ ë¬¸ì œ**: Fully Convolutional êµ¬ì¡°ë¡œ ì¸í•´ deep model ì‚¬ìš© ì‹œ ê³„ì‚° ë¹„ìš© ì¦ê°€ ê°€ëŠ¥ì„±
- **í´ë˜ìŠ¤ ê°„ í˜¼ë™ ë°œìƒ ê°€ëŠ¥**: í”½ì…€ ë‹¨ìœ„ ì˜ˆì¸¡ ë°©ì‹ìœ¼ë¡œ ì¸í•´ ê²½ê³„ê°€ íë ¤ì§€ëŠ” ë¬¸ì œ ë°œìƒ


**âœ…Â Contribution**

- FCN ê°œë… ë„ì…
- Skip Architectureë¥¼ í™œìš©í•œ Fine Segmentation ê°€ëŠ¥
- End-to-End í•™ìŠµì´ ê°€ëŠ¥í•œ Fully Convolutional êµ¬ì¡° ì œì•ˆ
- ê¸°ì¡´ SOTA ëŒ€ë¹„ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±
- ì‹¤ì‹œê°„ Semantic Segmentation ëª¨ë¸ ê°œë°œì— ê¸°ì—¬

</aside>

---

<aside>

<aside>

## **ğŸ“šÂ í•˜í–¥ì‹ ì ‘ê·¼**

> â„¹ï¸Â *ê³µí†µ ë…¼ë¬¸ í•™ìŠµ ì¤‘ ëª°ëë˜ ê°œë…, ìš©ì–´ì— ëŒ€í•œ ë…¼ë¬¸ì„ ì¶”ê°€ë¡œ ì°¾ì•„ë´…ì‹œë‹¤!*
> 
> 
> â„¹ï¸Â *í•œ ë„ë©”ì¸ì˜ ëª¨ë¥´ëŠ” ë‚´ìš©ì— ëŒ€í•´ **í•˜í–¥ì‹**ìœ¼ë¡œ ì ‘ê·¼í•˜ë‹¤ë³´ë©´ ê´€ë ¨ ë…¼ë¬¸ë“¤ì´ ì ì°¨ ì‰½ê²Œ ì½íˆê²Œ ë©ë‹ˆë‹¤.* 
> 
</aside>

**ğŸ“Â Skip Architecture**

**ğŸ“Â HHA Encoding** 

**ğŸ“Â Framework ì¢…ë¥˜(e.g. Caffe)**

**ğŸ“Â Per-pixel Multinomial Logistic Loss**

</aside>

---

<aside>

<aside>

## **ğŸ¤”Â Question**

> â„¹ï¸Â *ë³¸ì¸ì´ ìˆ˜í–‰í•œ í•™ìŠµì— ëŒ€í•´ ìŠ¤ìŠ¤ë¡œ ì§ˆë¬¸í•˜ê³  ë‹µí•´ë³´ì„¸ìš”.*
> 
> 
> **ğŸ“Â ì´ ë…¼ë¬¸ì´ ë“±ì¥í•˜ê²Œ ëœ ì´ìœ  + ì´ ë…¼ë¬¸ì´ ê´€ë ¨ Taskì— ê¸°ì—¬í•œ ë‚´ìš©**
> 
> **ğŸ“Â ë°°ìš¸ ìˆ˜ ìˆì—ˆë˜ ë‚´ìš©ê³¼ ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì **
> 
> **ğŸ“Â Git-Hubì— ê³µê°œëœ ì½”ë“œë¥¼ ë³´ê³  ì´í•´í•œ ë°” `(Opt.)`**
> 
</aside>

- ë“±ì¥í•œ ì´ìœ ?
: Patchwise trainingì˜ ë¬¸ì œì ì„ ë³´ì™„(ì´ë¯¸ì§€ ì „ì²´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” Fully Convolutional êµ¬ì¡° í•„ìš”),
 ê¸°ì¡´ ë°©ì‹ì˜ ì„±ëŠ¥(í•´ìƒë„, ì†ë„, ì—°ì‚°ëŸ‰ ë¬¸ì œ)ì„ ë†’ì´ê¸° ìœ„í•´ì„œ 
- Task ê¸°ì—¬?: ì´í›„ DeepLab, U-Net, PSPNet ë“± ë‹¤ì–‘í•œ Semantic Segmentation ëª¨ë¸ì˜ ë°œì „ì— í•µì‹¬ì ì¸ ì—­í• ì„ í•¨
- ë°°ìš´ ì ?: ê¸°ì¡´ CNN classification ëª¨ë¸ê³¼ì˜ ì°¨ì´ì , Skip connection architecture, Deconvoultionalì„ ì ìš©í•œ ì´ìœ , whole Image Training
- ì¶”ê°€ ê¶ê¸ˆì¦?: Semantic Segmentation ëª¨ë¸ì˜ êµ¬ì¡°ì™€ ë“±ì¥í•˜ê²Œ ëœ ì´ìœ ì— ëŒ€í•´ ì¢€ ë” ê³µë¶€í•´ë´ì•¼ê² ë‹¤.

</aside>

---

<aside>

<aside>

## **ğŸ¤”Â Next Task**

> â„¹ï¸Â *ì°¨ì£¼ì— ë¦¬ë·° ì˜ˆì •ì¸ ë…¼ë¬¸ ê¸°ì¬(APA. ì¸ìš© ë°©ì‹ ê¶Œê³ )*
> 
</aside>

- Zhao, H., Shi, J., Qi, X., Wang, X., & Jia, J. (2017). Pyramid scene parsing network. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2881-2890. https://doi.org/10.1109/CVPR.2017.660

</aside>

---
